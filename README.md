# Neural Networks â€“ XOR

This project implements a neural network **from scratch** in Python to solve the classic **XOR problem** â€” a fundamental test of nonlinear classification capability in neural networks.

---

## ğŸ§  What is the XOR Problem?

The XOR (exclusive OR) logic gate returns true if inputs differ (i.e., one is 1, the other is 0).

| Input A | Input B | XOR Output |
|---------|---------|------------|
|    0    |    0    |     0      |
|    0    |    1    |     1      |
|    1    |    0    |     1      |
|    1    |    1    |     0      |

A neural network must have **at least one hidden layer** with a **nonlinear activation function** to solve this.

---

## ğŸš€ Features

- Fully from scratch â€” no TensorFlow or PyTorch
- Custom implementation of:
  - Dense layers
  - Activation functions (ReLU, Sigmoid)
  - Loss function (Binary Cross-Entropy / MSE)
  - Backpropagation and weight updates
- Visual or print output to show learning

---

## ğŸ“‚ Project Structure

```
NN-XOR/
â”‚
â”œâ”€â”€ activation.py # Activation function interface
â”œâ”€â”€ activation_func.py # Sigmoid, ReLU, etc.
â”œâ”€â”€ dense.py # Dense (fully connected) layer
â”œâ”€â”€ layer.py # Layer base class
â”œâ”€â”€ loss.py # Loss function
â”œâ”€â”€ network.py # Neural network class
â”œâ”€â”€ xor.py # Entry point to train XOR
â””â”€â”€ README.md # You're reading it!
```

---

## ğŸ› ï¸ How to Run

```bash
python xor.py
```
## ğŸ§ª Output Example

```
Epoch 10000 - Loss: 0.0023
Predictions:
Input: [0, 0] â†’ 0.01
Input: [0, 1] â†’ 0.98
Input: [1, 0] â†’ 0.97
Input: [1, 1] â†’ 0.02

```
---

## ğŸ§° Requirements
```
Python 3.x

No external libraries (pure Python)
```

---

## âœï¸ Author
```
Created by Prasanth
```

